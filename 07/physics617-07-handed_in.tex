\documentclass[11pt, english, fleqn, DIV=15, headinclude, BCOR=1cm]{scrartcl}

\usepackage[bibatend]{../header}

\usepackage{lastpage}
\usepackage{multicol}
\usepackage{simplewick}
\usepackage{slashed}
\usepackage{subcaption}
\usepackage{tikzsymbols}
\usepackage{stmaryrd}

\usepackage{../my-boxes}

\newcommand\timeorder{\mathscr T}
\newcommand\normorder{\mathscr N}
\newcommand\eye{\mat 1_4}
\newcommand\myslash[1]{\underline{\slashed{\vec{#1}}}}
\newcommand\ensemble[1]{\llbracket #1 \rrbracket}
\newcommand\Ensemble[1]{\left\llbracket #1 \right\rrbracket}
\newcommand\sump{\sideset{}{'}\sum}

\hypersetup{
    pdftitle=
}

\graphicspath{{build/}}

\newcounter{totalpoints}
\newcommand\punkte[1]{#1\addtocounter{totalpoints}{#1}}

\newcounter{problemset}
\setcounter{problemset}{7}

\subject{physics617 -- Theoretical Condensed Matter Physics}
\ihead{physics617 -- Problem Set \arabic{problemset}}

\title{Problem Set \arabic{problemset}}

\newcommand\thegroup{Tutor: Ramsés Sánchez}

\publishers{\thegroup}
\ofoot{\thegroup}

\author{
    Martin Ueding \\ \small{\href{mailto:mu@martin-ueding.de}{mu@martin-ueding.de}}
}
\ifoot{Martin Ueding}

\ohead{\rightmark}

\begin{document}

\maketitle

\vspace{3ex}

\begin{center}
    \begin{tabular}{rrr}
        \toprule
        Problem & Achieved points & Possible points \\
        \midrule
        \nameref{homework:1} & & \punkte{5} \\
        \nameref{homework:2} & & \punkte{10} \\
        \nameref{homework:3} & & \punkte{15} \\
        \midrule
        Total & & \arabic{totalpoints} \\
        \bottomrule
    \end{tabular}
\end{center}

\vspace{3ex}

\begin{center}
    \begin{small}
        This document consists of \pageref{LastPage} pages.
    \end{small}
\end{center}

\section{Green's function of an impurity immersed in a fermionic bath}
\label{homework:1}

\subsection{Analytic in the upper half plane}

$V$ and $\epsilon_{\vec k}$ are real numbers. As long as the imaginary part of
the denominator does not vanish for $\Im(z) \neq 0$, the function is analytic.
We look at the imaginary part of the denominator:
\begin{align*}
    \Im(D)
    &= \Im\del{z - \epsilon - \sum_{\vec k} \frac{V^2}{z - \epsilon_{\vec k}}}
    \intertext{%
        The imaginary part is a linear operator and therefore we can directly
        move it part the terms that are purely real.
    }
    &= \Im(z) - \sum_{\vec k} V^2 \Im\del{\frac{1}{z - \epsilon_{\vec k}}}
    \intertext{%
        The imaginary part of a fraction might not be obvious right away. So we
        write it more explicitly:
    }
    &= \Im(z) - \sum_{\vec k} V^2 \Im\del{\frac{1}{\Re(z) + \iup \Im(z) - \epsilon_{\vec k}}}
    \intertext{%
        Then we make the denominator there real.
    }
    &= \Im(z) - \sum_{\vec k} V^2
    \Im\del{\frac{[\Re(z) - \epsilon_{\vec k}] - \iup \Im(z)}{[\Re(z) -
    \epsilon_{\vec k}]^2 + \Im(z)^2}}
    \intertext{%
        Now we can look at the imaginary part of that.
    }
    &= \Im(z) - \Im(z) \sum_{\vec k} V^2
    \frac{1}{[\Re(z) - \epsilon_{\vec k}]^2 + \Im(z)^2} \\
    &= \Im(z) \sbr{1 - \sum_{\vec k} V^2
    \frac{1}{[\Re(z) - \epsilon_{\vec k}]^2 + \Im(z)^2}}
\end{align*}

If the sum is sufficiently small, this works. It is not really clear that it
will work out, though.

\subsection{Residue at infinity}

The “residue” at infinity is
\begin{align*}
    \lim_{|z| \to \infty} z G(z)
    &=
    \frac{z}{z - \epsilon - \sum_{\vec k} \frac{V^2}{z - \epsilon_{\vec k}}}
    \,.
    \intertext{%
        We can cancel a $z$ in the denominator.
    }
    &=
    \lim_{|z| \to \infty}
    \sbr{1 - \frac\epsilon{z} - \sum_{\vec k} \frac{V^2}{z^2 - z\epsilon_{\vec
    k}}}\inv
\end{align*}
The denominator goes to one in the limit, therefore the whole fraction
is constant, namely unity.

\section{Yet another pair of frequency summations}
\label{homework:2}

\subsection{Frequency summation}

Perhaps the first part is not really needed as we have done a few frequency
summations already. I want to explain all the steps here in detail to show that
I understood the general principle and in order to have something to get up to
speed when studying for the exam in a month.

We want to evaluate a countable infinite sum over the fermionic Matsubara
frequencies~$\omega_n$ which are $[2n + 1] \piup / \beta$. In order to compute
this sum we want to use the residue theorem and construct a helper function
such that it has a pole for every summand. The function
\[
    \sbr{\exp(\beta z) + 1}\inv
\]
has poles at $z = \iup \omega_n$, which is exactly what we want. That function
is the Fermi function. In order to use this in our sum, we need the residues at
the poles. They are
\begin{align*}
    R_n &=
    \lim_{z \to \iup \omega_n} [z - \iup \omega_n] \sbr{\exp(\beta z) + 1}\inv
    \,.
    \intertext{%
        It is easier with $\epsilon := z - \iup \omega$. This leads us to
    }
    &= \lim_{\epsilon \to 0} \frac{\epsilon}{\exp(\beta \epsilon) \exp(\iup
    \beta \omega_n) + 1} \,.
    \intertext{%
        The factor $\exp(\beta\epsilon)$ can be linearized to $1 +
        \beta\epsilon$.
    }
    &= \lim_{\epsilon \to 0} \frac{\epsilon}{[1 + \beta\epsilon] \exp(\iup
    \beta \omega_n) + 1} \,.
    \intertext{%
        Since the fermionic Matsubara frequencies are “odd”, the exponential
        will always just give a minus sign. Therefore the denominator is just
    }
    &= \lim_{\epsilon \to 0} \frac{\epsilon}{1 - 1 - \beta\epsilon} \\
    &= - \lim_{\epsilon \to 0} \frac{\epsilon}{\beta\epsilon} \\
    &= - \frac{1}{\beta} \,.
\end{align*}
Therefore the residue is just $-\beta\inv$. This can easily be accounted for in
the transformation of the sum into a contour integral.

The Fourier transformed Matsubara Green's function for non-interacting
particles was given on the sixth homework sheet as
\[
    \tilde G^{(0)}(\vec k, \vec \xi_n) = \frac{1}{\iup \xi_n - \epsilon_{\vec
    k}} \,.
\]
We start with the expression to compute. The left hand side is
\begin{align*}
    \text{LHS}
    &=
    - \frac1\beta \sum_n
    \tilde G^{(0)}(\vec p, \omega_n)
    \tilde G^{(0)}(\vec k, \nu_m - \omega_n) \,.
    \intertext{%
        We insert the function from the previous exercise sheet.
    }
    &= - \frac1\beta \sum_n
    \frac{1}{\iup \omega_n - \epsilon_{\vec p}}
    \frac{1}{\iup \nu_m - \iup \omega_n - \epsilon_{\vec k}}
    \intertext{%
        As derived above, the function $n_\text F(z)$ will give us the correct
        residues for $z = \iup \omega_n$. Therefore we can replace the sum with
        a contour integral. The contour will be along a narrow strip around the
        imaginary axis to include only the poles at $\iup \omega_n$ and none of
        the other poles. The factor $- \beta\inv$ will now come from the
        residuals. This transforms the expression to
    }
    &= \frac{1}{2 \piup \iup} \ointop_{\text{narrow}}
    \dif z \,
    \frac{1}{z - \epsilon_{\vec p}}
    \frac{1}{\iup \nu_m - z - \epsilon_{\vec k}}
    \frac{1}{\exp(\beta z) + 1} \,.
    \intertext{%
        We do not need the explicit form of the Fermi function, so we just
        replace that with the name.
    }
    &= \frac{1}{2 \piup \iup} \ointop_{\text{narrow}}
    \dif z \,
    \frac{1}{z - \epsilon_{\vec p}}
    \frac{1}{\iup \nu_m - z - \epsilon_{\vec k}}
    n_\text F(z)
    \intertext{%
        The idea now is the same as usual. The contour tightly wrapped around
        the imaginary axis will give the desired result. To compute it we need
        to add all the residues, something we would like to get around with.
        If we push the contour to infinity, the integral itself will go to zero
        as the integrand decays with $z^{-2}$. In doing so we pick up the poles
        of the two fractions in the integrand. So in order to account for that
        we have
    }
    &= \frac{1}{2 \piup \iup}
    \sbr{
        \ointop_{\text{great circle}}
        -
        \ointop_{\text{other poles}}
    }
    \dif z \,
    \frac{1}{z - \epsilon_{\vec p}}
    \frac{1}{\iup \nu_m - z - \epsilon_{\vec k}}
    n_\text F(z) \,.
    \intertext{%
        By the argument given above the contour integral along the great circle
        has to vanish. Therefore we are left with
    }
    &= - \frac{1}{2 \piup \iup} \ointop_{\text{other poles}}
    \dif z \,
    \frac{1}{z - \epsilon_{\vec p}}
    \frac{1}{\iup \nu_m - z - \epsilon_{\vec k}}
    n_\text F(z) \,.
    \intertext{%
        Now this is easy. Just locate the remaining poles and add the residues.
        The Fermi function does not have any poles with a real part, therefore
        the residue will just be the Fermi function at the pole. Those poles
        are $z = \epsilon_{\vec p}$ and $z = \iup \nu_m - \epsilon_{\vec p}$.
        The residues of the fractions are both $[\iup \nu_m - [ \epsilon_{\vec
        p} + \epsilon_{\vec k}]]\inv$ which is a nice coincidence. The value of
        the Fermi function is different in each case. In total we have
    }
    &= \frac{- n_\text F(\epsilon_{\text p}) - n_\text F(\iup \nu_m - \epsilon_{\vec
    k})}{\iup \nu_m - \epsilon_{\vec p} - \epsilon_{\vec k}}
    \,.
    \intertext{%
        This looks very good, the argument of the second Fermi function needs
        some adjustment, though. Using the trick to be shown below, we obtain
        the final result of
    }
    &= \frac{-1 - n_\text F(\epsilon_{\text p}) + n_\text F(\epsilon_{\vec
    k})}{\iup \nu_m - \epsilon_{\vec p} - \epsilon_{\vec k}}
    \,.
\end{align*}
There is a sign mistake somewhere, though. Perhaps one of the residues has a
different sign, then it would work out. Now the trick mentioned above. Using
the definition of the Fermi function with the expression which contained the
negative sign we have
\begin{align*}
    n_\text F(\iup \nu_m - \epsilon_{\vec k})
    &= \frac{1}{\exp(\iup \beta \nu_m - \beta \epsilon_{\vec k}) + 1} \,.
    \intertext{%
        Then we just split the exponential to give
    }
    &= \frac{1}{\exp(\iup \beta \nu_m) \exp(- \beta \epsilon_{\vec k}) + 1} \,.
    \intertext{%
        The boson frequencies are integer multiples of $2 \piup/\beta$ and
        therefore just give $\exp(2 \piup \iup n) = 1$. We can drop the term
        and are left with
    }
    &= \frac{1}{\exp(- \beta \epsilon_{\vec k}) + 1} \,.
    \intertext{%
        Then we expand the fraction with $\exp(\beta \epsilon_{\vec k})$ to get
        rid of the minus sign. We then have
    }
    &= \frac{\exp(\beta \epsilon_{\vec k})}{1 + \exp(\beta \epsilon_{\vec k})}
    \,.
    \intertext{%
        In the numerator we add zero to have
    }
    &= \frac{1 + \exp(\beta \epsilon_{\vec k}) - 1}{1 + \exp(\beta
    \epsilon_{\vec k})} \,.
    \intertext{%
        From here we can split up the two fractions in a trivial way
    }
    &= \frac{1 + \exp(\beta \epsilon_{\vec k})}{1 + \exp(\beta
    \epsilon_{\vec k})} - \frac{1}{1 + \exp(\beta \epsilon_{\vec k})}
    \intertext{%
        and reduce the first fraction. We then have
    }
    &= 1 - \frac{1}{1 + \exp(\beta \epsilon_{\vec k})} \,,
    \intertext{%
        which can be written in terms of the Fermi function again to give
    }
    &= 1 - n_\text F(\epsilon_{\vec k}) \,.
\end{align*}

\subsection{Limit}

In the problem set the momentum is set to zero, although on the right hand side
a vector $\vec k$ comes up. We will use a vector $\vec X$ which would be just
the zero vector on the problem set. We expand the momentum-time Green's function as
a momentum-frequency Green's function first. This give us
\begin{align*}
    \lim_{\tau \nearrow 0} G^{(0)}(\tau, \vec X)
    &= \lim_{\tau \nearrow 0} \frac 1\beta \sum_{n} \exp(- \iup \omega_n \tau)
    \tilde G^{(0)}(\omega_n, \vec X) \,.
    \intertext{%
        The exponential on the problem set lacks a factor $\tau$, we have just
        added it here to make sense of the units. This is just another
        frequency summation problem. We insert $\tilde G^{(0)}$ explicitly and
        obtain
    }
    &= \lim_{\tau \nearrow 0} \frac 1\beta \sum_{n} \exp(- \iup \omega_n \tau)
    \frac1{\iup \omega_n - \epsilon_{\vec X}} \,,
    \intertext{%
        As before we rewrite this as a contour integral with a narrow curve
        around the imaginary axis. This is
    }
    &= - \lim_{\tau \nearrow 0} \ointop_\text{narrow}
    \frac{\dif z}{2 \piup \iup} \, \exp(- z \tau)
    \frac{n_\text F(z)}{z - \epsilon_{\vec X}} \,,
\end{align*}
Only at this point it is apparent that the limit $\tau \nearrow 0$ does
something useful. If we had performed the limit already then the
exponential factor was gone. The denominator only decays with $z\inv$
such that the total integral over a great circle does not vanish without any
such help. If we do not care about convergence and just do the contour
integration we would have to take the pole at $z = \epsilon_{\vec X}$ into
account. The residue at this pole is just
\[
    \lim_{\tau \nearrow 0} \exp(- \epsilon_{\vec X} \tau) \,
    n_\text F(\epsilon_{\vec X}) \,.
\]
In the limit $\tau \nearrow 0$ we just have $n_\text F(\epsilon_{\vec X})$. If
that $\vec X$ had been $\vec k$ all the way, this would be close to the correct
result. We cancel the minus sign since we need to subtract this pole from the
“great circle” contour. We do not know that this “great circle” contour
integral will give zero, so one cannot really say that this works out here.

\section{Wick's theorem for non-operators}
\label{homework:3}

The location of the impurities is defined as
\[
    \rho_{\vec q} := \sum_i \exp(- \iup \vec q \cdot \vec R_i) \,.
\]

\subsection{Ensemble averages}

The definition of the ensemble average given in the lecture is
\[
    \ensemble G := \int \sbr{\prod_{i=1}^N \dif^3 R_i} G(\vec R_1, \dotsc, \vec R_N) \,.
\]
The notation is a bit different here. First I use some other brackets. There
are double angle brackets in \texttt{MnSymbol} but that clashes with
\texttt{amssymb} and I do not want to get those to work right now. Those are
taken from \texttt{stmaryrd} which plays nice with \texttt{amssymb}. Then I
like to have function evaluation with round brackets no matter what the
arguments are. From a programming perspective I find it illogical that the
function arguments should be enclosed in square brackets if one of them is
another function. And because the theorist convinced me that writing the
differential forms (or measure or integration variable) up front makes more
sense, I will do that here as well.

It is important that the integration only applies to the actual values of $\vec
R$ used. Otherwise they would create infinite multiplicative factors which is
not the desired outcome. With that in mind we can start to evaluate some
averages.
\begin{align*}
    \ensemble{\rho_{\vec q}}
    &= \Ensemble{\sum_i \exp(- \iup \vec q \cdot \vec R_i)} \\
    \intertext{%
        The ensemble average is linear so we can pull out the sum.
    }
    &= \sum_i \Ensemble{\exp(- \iup \vec q \cdot \vec R_i)} \\
    \intertext{%
        Each of the terms is a functional of only one of the $\vec R$ now.
        Therefore we only need one integration.
    }
    &= \sum_i \int \dif^3 R_i \, \exp(- \iup \vec q \cdot \vec R_i)
    \intertext{%
        This integration just gives a $\delta$-distribution (perhaps with
        another factor of $2 \piup$?) and we have
    }
    &= \sum_i \delta(\vec q) \,.
    \intertext{%
        All the $N$ summands are equal, therefore this reduces to
    }
    &= N \delta(\vec q) \,.
\end{align*}
This matches the expression given in the lecture. There, both $n$ and $N$ are
used for the same thing I assume, so I will go with $N$ for everything.

The two-density expression can be computed similarly. We start with
\begin{align*}
    \ensemble{\rho_{\vec q_1} \rho_{\vec q_2}}
    &=: \ensemble{\rho_1 \rho_2}
    \intertext{%
        where we have introduced some shorthand notation. Next we write out the
        sums and have
    }
    &= \sum_{i, j} \Ensemble{\exp(- \iup \vec q_1 \cdot \vec R_i) \exp(- \iup
    \vec q_2 \cdot \vec R_j)} \,.
    \intertext{%
        The cases $i = j$ and $i \neq j$ have to be treated differently. We
        introduce the notation $\sum'$ which means that only unique elements
        are summed over. With that notation we have
    }
    &= \sum_{i} \Ensemble{\exp(- \iup [\vec q_1 + \vec q_2] \cdot \vec R_i)}
    +
    \sump_{i, j} \Ensemble{\exp(- \iup \vec q_1 \cdot \vec R_i) \exp(- \iup
    \vec q_2 \cdot \vec R_j)}
    \,.
    \intertext{%
        The first summand is something we have just calculated, it is just $N
        \delta(\vec q_1 + \vec q_2)$. In the second summand, we have two
        distinct integrals.
    }
    &= N \delta(\vec q_1 + \vec q_2)
    +
    \sump_{i, j}
    \int \dif^3 R_i \, \exp(- \iup \vec q_1 \cdot \vec R_i)
    \int \dif^3 R_j \, \exp(- \iup \vec q_2 \cdot \vec R_j)
    \intertext{%
        Each of the integrals gives a $\delta$-distribution.
    }
    &= N \delta(\vec q_1 + \vec q_2)
    + \sump_{i, j} \delta(\vec q_1) \delta(\vec q_2)
    \intertext{%
        There are $N[N-1] = N^2 - N$ summands in the second term. In the
        lecture, it was shown to be $N^2$, though. Perhaps it is because $N \gg
        1$ and therefore this can be approximated to be $N^2$ directly? We will
        just use this argument to obtain the result in the lecture on
        2015-12-01 which is the canonical result.
    }
    &= N \delta(\vec q_1 + \vec q_2)
    + N^2 \delta(\vec q_1) \delta(\vec q_2)
\end{align*}

From here the systematics are clear, there are just more combinations in the
case of three terms. We introduce even more shorthand notation. We write
\begin{align*}
    \ensemble{\rho_1 \rho_2 \rho_3}
    &= \sum_{i, j, k} \Ensemble{\rho_1^i \rho_2^j \rho_3^k}
    \intertext{%
        where the upper indices are the index of the $\vec R$ that appears in
        the exponential. As seen above we need to split this up into cases
        where some of the indices are the same.
    }
    &= \sump_{i, j, k} \Ensemble{\rho_1^i \rho_2^j \rho_3^k}
    + \sump_{i, j} \Ensemble{\rho_1^i \rho_2^j \rho_3^j}
    + \sump_{i, j} \Ensemble{\rho_1^j \rho_2^j \rho_3^i}
    \\&\qquad
    + \sump_{i, j} \Ensemble{\rho_1^j \rho_2^i \rho_3^j}
    + \sum_{i} \Ensemble{\rho_1^i \rho_2^i \rho_3^i}
    \intertext{%
        By looking at the above calculations, we can just write down the
        results.
    }
    &= N^3 \delta(\vec q_1) \delta(\vec q_2) \delta(\vec q_3)
    + N^2 \delta(\vec q_1 + \vec q_2) \delta(\vec q_3)
    + N^2 \delta(\vec q_1 + \vec q_3) \delta(\vec q_2)
    \\&\qquad
    + N^2 \delta(\vec q_1) \delta(\vec q_2 + \vec q_3)
    + N \delta(\vec q_1 + \vec q_2 + \vec q_3)
\end{align*}

\subsection{Feynman diagrams}

Finally I can create some Feynman diagrams with Ti\emph{k}Z for this course as
well! \Laughey The \emph{Advanced Quantum Field Theory} course has lots of
opportunities to make those. Now you will not be spared with them either.
\Winkey

An impurity cannot absorb any momentum in the case where the positions are
averaged over. Therefore it can only “reflect” momentum which leads to the
momentum sum $\delta$-distributions. In analogy to the diagrams shown in the
lecture we can construct diagrams to correspond to each of the terms in the
previous subsection. Table~\ref{tab:single} shows the Feynman diagrams for the
scattering on a single impurity.

\begin{table}
    \centering
    \begin{tabular}{cc}
        \toprule
        Term & Feynman Diagram \\
        \midrule
        $\delta(\vec q_1)$ & \includegraphics{one} \\
        \midrule
        $\delta(\vec q_1 + \vec q_2)$ & \includegraphics{two} \\
        \midrule
        $\delta(\vec q_1 + \vec q_2 + \vec q_3)$ & \includegraphics{three} \\
        \bottomrule
    \end{tabular}
    \caption{%
        Feynman diagrams with scattering on a single impurity. The impurity is
        denoted with a box as Ti\emph{k}Z does not seem to have a pre-made
        shape which resembles the cross used in the lecture.
    }
    \label{tab:single}
\end{table}

Multiple of those single impurity Feynman diagrams can be combined when
multiple impurities are considered in a scattering process.
Figure~\ref{fig:iij} shows one of the mixed terms from the previous
computation, the term with
\[
    \sump_{i, j} \Ensemble{\rho_1^j \rho_2^j \rho_3^i} \,.
\]

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.48\linewidth}
        \centering
        \includegraphics{iij}
        \caption{%
             Diagram corresponding to $\ensemble{\rho_1^j \rho_2^j \rho_3^i}$
             or $\contraction{}1{}2 123$.
        }
        \label{fig:iij}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\linewidth}
        \centering
        \includegraphics{mixed}
        \caption{%
             Diagram corresponding to $\ensemble{\rho_1^j \rho_2^i \rho_3^j}$
             or $\contraction{}1{2}3 123$.
        }
        \label{fig:iji}
    \end{subfigure}
    \caption{%
        Feynman diagrams for scattering three times on two impurities.
        Interference is not excluded with the current formalism. The diagrams
        only differ the be order of the indices.
    }
    \label{fig:iji/mixed}
\end{figure}

\subsection{$n$-th order}

The emerging pattern is that all cases of equal and unequal indices must be
taken into account. That can be thought of as indices being coupled or
independent. Actually they are not really independent as they must not equal
each other, but that seems to be glossed over in the $N - 1 \approx N$
approximation. We can write those coupling similar to the Wick contractions in
the correlation functions. Here we can have uncoupled or groups with more than
two elements. Writing $\rho_1 \rho_2 \rho_3$ as a super-short $123$
we can express this as
\[
    \ensemble{123}
    = 123
    +
    \contraction{1}2{}3
    123
    +
    \contraction{}1{}2
    123
    +
    \contraction{}1{2}3
    123
    +
    \contraction{1}2{}3
    \contraction{}1{}2
    123 \,.
\]
Those are the five terms that we have computed above. All numbers that are not
contracted are a scattering with a single impurity.

\end{document}

% vim: spell spelllang=en tw=79
